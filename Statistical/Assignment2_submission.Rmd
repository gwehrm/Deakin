---
title: "Assignment2_submission"
author: "Gabor Wehrmuller - 219369109"
date: "07/09/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
library(mgcv)
library(rnoaa)

```

# To do:

2.2 Check when the weather data is updated -
2.4 What exactly is asked - statistical methods? What is exactly used to estimate the parameters in GAMs 
3.4 report through coefficient estimates or plots?
3.6 but dont we specify before as well ordered weekdays?
tidy output with broom
3.6 check out what by does 

 
# Task 1 Source Weather Data

## 1. Which data source do you plan to use? Justify your decision.
The original data stems from weather stations. They are usually run by the local government. However the collection and storing of the data can be done by different providers. I plan to use NOAA, the National Centers for Environmantal Information, a US-based data center. AlsoI gather data from the Autralian Bureau of Meteorology. 

Additionaly the data from last assignment, the ED-Demand stems from the Autralian government website. 
Generally government run databases are a good source, since they are free and reliable.

It is asked to get data to predict the ED-Demand. The time period to predict is not given. I assume that the task is to predict the demand for the next day.

After a quick research online, the relevant feature preliminary related to temeperature. ALso I included data about the solar exposure. Originaly I wanted to include data about air pollution as well. But only data about emissions of facilities was available. Therefore I didnt use that. 

## 2.

```{r noaa_data}
# library for access to noaa


# set key
options(noaakey = "aeiJGRXtNXdMuIkEVeqOvciRvsdBbIUg")

# get the id for perth
x <- ncdc_locs(locationcategoryid='CITY', sortfield='name', sortorder='desc',limit = 1000)$data
perth <- x[x$name=="Perth, AS",]
# id:	CITY:AS000008

# get the weatherstations and select the one with the highest coverage
stations <- ncdc_stations(datasetid='GHCND',
                          locationid='CITY:AS000008')$data
#x perth airport GHCND:ASN00009021

# check which datasets have perth data
ncdc_datasets(locationid = "CITY:AS000008")
# GHCND is here the only source

# the possible data are max, min, avg and precipitation
max_temp_daily <- ncdc(datasetid='GHCND',
                       stationid='GHCND:ASN00009021',
                       datatypeid='TMAX',
                       startdate = '2013-07-01',
                       enddate = '2014-06-30',
                       limit=500)$data
min_temp_daily <- ncdc(datasetid='GHCND',
                       stationid='GHCND:ASN00009021',
                       datatypeid='TMIN',
                       startdate = '2013-07-01',
                       enddate = '2014-06-30',
                       limit=500)$data
avg_temp_daily <- ncdc(datasetid='GHCND',
                       stationid='GHCND:ASN00009021',
                       datatypeid='TAVG', 
                       startdate = '2013-07-01',
                       enddate = '2014-06-30',
                       limit=500)$data
prec_daily <-ncdc(datasetid='GHCND',
                  stationid='GHCND:ASN00009021',
                  datatypeid='PRCP',
                  startdate = '2013-07-01',
                  enddate = '2014-06-30', 
                  limit=500)$data

# change date and select the important variables

change_noaadate <- function(df){
  require(tidyverse)
  df_2 <- df %>%
    dplyr::mutate(date = as.Date(substr(date,1,10)))%>%
    select(date,value)
  names(df_2) <- c("date",df$datatype[1])
  return(df_2)
}

max_temp_daily <- change_noaadate(max_temp_daily)
min_temp_daily <- change_noaadate(min_temp_daily)
avg_temp_daily <- change_noaadate(avg_temp_daily)
prec_daily <- change_noaadate(prec_daily)

dim(max_temp_daily)

```

In the 4 dataframes, there are in each 365 rows. I chose the time period to be the same as in the ed-demand data, which is 2013-07-01 to 2014-06-30. 


 

# Task 2: Model planning

## 1. 
The model could play an important role for hospitals to control the ED-demand. Usually people have to wait for a long time till they get treatment at an hospital emergency departement. With an accurate prediction about the demand in hospitals, people could see in which hospital the demand is higher than in others and the patient could better chose where to go to minimise waiting time. 

Not only is it a problem, that patients are not well allocated, but also that often there are too many of them at the ED, who are not as seriously in danger that they could just make an appointment with a doctor next week. If people are more aware of the possible waiting times at ED, some of them might wait and make an normal appointment. 

Finally, this predictions can also be valuable for the hospitals personal planning. To better plan when it is suitable for nurses etc to take vacations.

Overall this model can be used by the people for choosing the hospital with the least waiting time, or they see that theyjust wait for an appointment and finally also for personal planning at hospitals. 


## 2.
As I have outlined in 1. There might be 2 applications. One when the possible patient tries to decide to go to which hospital and another when the hospitals want to make their planning more efficient. For the planning, a prediction overa longer period of time is helpful. But for the possible patient, a daily prediction is what he wants. 

So I want to focus only on the daily prediction for the patient. In this case the ED-Demand, the Attendance is the dependent variable. The predictors are the climate data from the day before. 

It seems that for a daily prediction, the NOAA database is not suitable

## 3.

One could assumee that climate data shows a reliable pattern over the course over a year. However, with increasing changes due to the climate change, this might not be true any longer. 

## 4.
The dependent variable is numeric. Therefore, it will be a regression problem. By starting at a simple linear regression one can go on to more comoplex models if necessary. The methods used to estimate the parameters are in a linear regression the Ordinary Least Squares (OLS). When we would go on to the Generalized Addiditves Models, the method used for the parameter estimation would be the IRLS. 



# Task 3
## 1.

### ED-Data

```{r ed_data, message=FALSE, warning=FALSE}

# code from assignmnet
ed_link <- "http://bit.ly/2nkCUEh" #changed to link

top_row <- read_csv(ed_link, col_names = FALSE, n_max = 1)
second_row <- read_csv(ed_link, n_max = 1)

column_names <- second_row %>% 
  unlist(., use.names=FALSE) %>% 
  make.unique(., sep = "__") # double underscore

column_names[2:8] <- str_c(column_names[2:8], '0', sep='__')

daily_attendance <- 
  read_csv(ed_link, skip = 2, col_names = column_names,na = "N/A")


```


```{r tidy_ed}
# tidy the data
daily_attendance_tidy <-  daily_attendance %>%
  gather(-Date, key = key, value = value)%>% # gather all collumns except date
  tidyr::extract(key, c("category", "hospital_number"), "(\\w*)(\\d)", convert = TRUE)%>% # extract the hospital numbers
  mutate(category = str_sub(category,1,-3))%>%
       #  value = replace_na(value,0))%>%
  spread(category, value)%>%
  mutate(Hospital = case_when(hospital_number == 0 ~"Royal Perth Hospital",
                              hospital_number == 1 ~"Fremantle Hospital",
                              hospital_number == 2 ~"Princess Margaret Hospital For Children",
                              hospital_number == 3 ~"King Edward Memorial Hospital For Women",
                              hospital_number == 4 ~"Sir Charles Gairdner Hospital",
                              hospital_number == 5 ~"Armadale/Kelmscott District Memorial Hospital",
                              hospital_number == 6 ~"Swan District Hospital",
                              hospital_number == 7 ~"Rockingham General Hospital",
                              hospital_number == 8 ~"Joondalup Health Campus"))%>%
  dplyr::select(-hospital_number) # remove column
# replace all NAs in df with 0 - justification see Ass 1
daily_attendance_tidy[is.na(daily_attendance_tidy)] <- 0
```


```{r}
# to obtain the list of hospitals
set.seed(123)# to get the same results
hospitals <- top_row %>%
  select_if(~!is.na(.))

# print random Hospital
print(sample(as.vector(hospitals),size = 1))
```
Princess Margaret Hospital For Children is the selected hospital.
I assume that the next step is to filter the ED-date for this particlular hospital and continue with that data the assignment.

```{r}
Margaret_ed <- daily_attendance_tidy%>%
  filter(Hospital == "Princess Margaret Hospital For Children")%>%
  mutate(date = dmy(Date))

```





## 2. 
```{r}
require(lettuce)
fit_linear <- lm(Attendance~date, data = Margaret_ed) 
# linear regression
summary(fit_linear)

Margaret_ed%>%
  ggplot(aes(x = date,y = Attendance))+
  geom_point()+
  geom_smooth(method = "lm")

# 
# res <- stack(data.frame(Observed = Margaret_ed$Attendance , Predicted = fitted(fit_linear)))
# res <- cbind(res, x = rep(Margaret_ed$date, 2))
# 
# require("lattice")
# 
# xyplot(values ~ x, data = res, group = ind, auto.key = TRUE)
# 
plot(fit_linear$residuals)
# 

plot(fit_linear)
```
From the regression output, It can be seen that the linear fit is really bad. Only 0.3% of the variation can be explained. by "date".

The residualplots indicate that there is some seasonality that we are not capturing with this linear fit. So this linear regression is defenitely not good enough. A next step could be, if we want to continue with the linear models, to incroporate polynomials, that give more flexibility.
```{r}
fit_poly <- lm(Attendance~poly(date,4), data = Margaret_ed) 
summary(fit_poly)
plot(fit_poly)
```
The fit increased already dramatically. A vote for more flexibility.


## 3. 
```{r}
fit_gam <- gam(Attendance~s(as.numeric(date), k = 50),
               data = Margaret_ed)
summary(fit_gam)
plot(fit_gam, residuals = TRUE)
gam.check(fit_gam)
```
The fit increased and much more can be explained. The adjusted R-sqared is 0.412. But by just looking atthe residuals, it cannot be said that there is something way off - bad
family = norma, since normally distributed value...


## 4.
 
```{r}
Margaret_ed_week <- Margaret_ed%>%
  mutate(weekd = as.factor(weekdays(date)),
         num_date  =as.numeric(date))

# str(Margaret_ed_week)

fit_gam_week <- gam(Attendance ~s(num_date, by  = weekd) +weekd,
                    data = Margaret_ed_week)
summary(fit_gam_week)

AIC(fit_gam,fit_gam_week)


plot(fit_gam_week, residuals = T)

```
The AIC reveals that the second model, fit_gam_week has a better fit than the first model. 

since as.factor, we stil lhave some kind of order 

## 5. 


```{r}
plot(fit_gam_week, residuals = T)
gam.check(fit_gam_week)
```
the residuals still look nicely distributed. 


## 6. 

```{r}
# add the ordinal  and numeric
Margaret_ed_week <- Margaret_ed%>%
   mutate(weekd = as.character(weekdays(date)),
         weekd_or = factor(weekdays(date), order = T, levels = c("Monday","Tuesday","Wednesday","Thursday","Friday","Saturday","Sunday")),
         weekd_nu = lubridate::wday(date),
         num_date  =as.numeric(date))

# character
fit_gam_week <- gam(Attendance ~s(num_date) +weekd,
                    data = Margaret_ed_week)
summary(fit_gam_week)

# ordinal
fit_gam_week_o <- gam(Attendance ~s(num_date, by  = weekd_or) +weekd_or,
                    data = Margaret_ed_week)
summary(fit_gam_week_o)

# numeric
fit_gam_week_n <- gam(Attendance ~s(num_date, by  = weekd_nu) +weekd_nu,
                    data = Margaret_ed_week)
summary(fit_gam_week_n)



AIC(fit_gam_week,fit_gam_week_o,fit_gam_week_n)

```
There is a big difference in between the models. The presumably unordered model is the best. But why?
It makes sense that the ordered and the factors are similar. THey have the same order and with the structure and flexibility of gams, both capture the same structure. Therefore they are similar. But why is the first one the best? Does it allow the highest flexibility? 




# Task 4 

## 1. The Definition of EHF
The EHF is calculated in multiple steps and tries to capture and indicate heatwaves.

First step is to calculate the excess heat indndices:

$EHI_{sig} = (T_i+T_{i+1}+T_{i+2})/3-T_{95}\;,where \;T\ is \;Daily\;Mean\;Temperature\; and \;T_{95} \;95 \; percentile$
and:
$EHI_{accl}=(T_i+T_{i+1}+T_{i+2})/3-(T_{i-1}+...+T_{i-30})/30 $

The EFH can be calculated as followed:

$EHF = EHI_{sign}*max(1,EHI_{accl})$
or
$EHF = max(0,EHI_{sign})*max(1,EHI_{accl})$


Knowing this, additional data is required:
* 30 additional days, so start is now 2013-06-01
* years 1971–200 to caclulate $T_{95}$


```{r merge_data}

calc_t95 <- function(timesspan){
  require(rnoaa)
  avg_temp <-  c()
  for (i in timesspan){
    start <-  paste0(i,"-01-01")
    end <-  paste0(i,"-12-31")
    
    avg_temp_daily <- ncdc(datasetid='GHCND',
                         stationid='GHCND:ASN00009021',
                         datatypeid='TAVG', 
                         startdate = start,
                         enddate = end,
                         limit=500)$data
    if (length(avg_temp_daily)==0){
      next
    }
    avg_temp <- append(avg_temp, avg_temp_daily$value)
  }
  return(quantile(avg_temp,.95))
  }


# the possible data are max, min, avg and precipitation
max_temp_daily_long <- ncdc(datasetid='GHCND',
                       stationid='GHCND:ASN00009021',
                       datatypeid='TMAX',
                       startdate = '2013-07-01',
                       enddate = '2014-06-30',
                       limit=500)$data
min_temp_daily_long <- ncdc(datasetid='GHCND',
                       stationid='GHCND:ASN00009021',
                       datatypeid='TMIN',
                       startdate = '2013-07-01',
                       enddate = '2014-06-30',
                       limit=500)$data
avg_temp_daily_long <- ncdc(datasetid='GHCND',
                       stationid='GHCND:ASN00009021',
                       datatypeid='TAVG', 
                       startdate = '2013-07-01',
                       enddate = '2014-06-30',
                       limit=500)$data
prec_daily_long <-ncdc(datasetid='GHCND',
                  stationid='GHCND:ASN00009021',
                  datatypeid='PRCP',
                  startdate = '2013-07-01',
                  enddate = '2014-06-30', 
                  limit=500)$data


max_temp_daily_long <- change_noaadate(max_temp_daily_long)
min_temp_daily_long <- change_noaadate(min_temp_daily_long)
avg_temp_daily_long <- change_noaadate(avg_temp_daily_long)
prec_daily_long <- change_noaadate(prec_daily_long)


margaret_weather_long <- Margaret_ed_week%>%
  mutate(date = dmy(Date))%>%
  left_join(max_temp_daily_long,by = "date")%>%
  left_join(min_temp_daily_long,by = "date")%>%
  left_join(avg_temp_daily_long,by = "date")%>%
  left_join(prec_daily_long,by = "date")%>%
  arrange(date)


calc_ehf <- function(df,timespan){
  require(zoo)
  df$EHF <- 1
  
  # EHI_sig
  t_95 <- calc_t95(timespan) # calculate 95 percentile
  # check to get the whole dataset of avg temperature to calculate it
  rolled_mean_f <- c(rollmean(df$TAVG,3),NA,NA) #= first try add NA at the end
  EHI_sig <- rolled_mean_f-t_95
  
  # EHI_accl
  rolled_mean_b <- c(rep(NA,29), rollmean(df$TAVG[length(df$TAVG):1],30))
 EHI_accl <- rolled_mean_f/rolled_mean_b
 for (i in 1:365){
  df$EHF[i] <- max(0,EHI_sig[i])*max(1,EHI_accl[i])
 }
 return(df) 
}
```



```{r}

timesspan <- 1970:2000

margaret_weather_EHF <- calc_ehf(margaret_weather_long, timesspan)

ggplot(margaret_weather_EHF, aes(x = date, y = EHF))+geom_line()

```

## Task 4.2 

```{r}

fit_gam_ehf <- gam(Attendance ~s(num_date, by  = weekd) +weekd+s(EHF),
                    data = margaret_weather_EHF)
summary(fit_gam_ehf)

AIC(fit_gam_week,fit_gam_ehf)


```


The EHF increases the fit again. The model performs better and can better explain the variation, as the AIC and the adj R-square confirm. 

What is the coefficient? 





```{r, ed_weather}
ed_weather_data%>%
  mutate(week = strftime(date, format = "%V"))%>%
  group_by(week)%>%
  summarize(weekly_att=sum(Attendance))%>%
  ggplot(aes(week,weekly_att, group =1))+
  geom_line()+
  theme(axis.text.x = element_text(angle = 45, size = 10))
  
```


## Task 4.3

```{r sun_data}
sun_daily <- read_csv("C:/Users/gwehrm/Documents/Repos/Deakin/Statistical/Data/IDCJAC0016_009021_1800_Data.csv")

sun_daily <- sun_daily%>%
  unite(date ,c(Year,Month,Day), sep = "-",remove = T)%>%
  mutate(date = as.Date(date))%>%
  filter(between(date, as.Date("2013-07-01"),as.Date("2014-06-30")))%>%
  select(date, `Daily global solar exposure (MJ/m*m)`)%>%
  rename(solar = `Daily global solar exposure (MJ/m*m)`)

margaret_weather_s <- Margaret_ed_week%>%
  mutate(date = dmy(Date))%>%
  left_join(max_temp_daily,by = "date")%>%
  left_join(min_temp_daily,by = "date")%>%
  left_join(avg_temp_daily,by = "date")%>%
  left_join(prec_daily,by = "date")%>%
  left_join(sun_daily, by = "date")%>%
  arrange(date)

margaret_weather_s <- calc_ehf(margaret_weather_s)


fit_gam_s <- gam(Attendance ~s(num_date, by  = weekd) +weekd+s(EHF)+s(solar)+s(TMIN),
                    data = margaret_weather_s)
summary(fit_gam_s)

AIC(fit_gam_ehf,fit_gam_s)



```

# Task 5
## 1 limitations of historical data

The Example of the EHF can show clearly some limitaitons. With increasing temperature more of the data will be labelled as heatwave, since as comparison historical data is used. Meanwhile people could adapt themselves to the increased heat and be therefore less sensitive to the EHF callculated with historical fata.

## 2. 
Well originally the task was to predict the ED-demand. But given multiple factors, the understanding of the process should be more in the focus. One reason is that only data for a year is available. Also, the data is from 2014. As said in 1., a limitation of historical data is that the behaviour and the relation can change. The data is nearly six years old and therefore not suitable to use for predictions in 2019/2020.

But insights forom this models can be used to the incorporate features in the prediction model.

## 3.
No not overall. The original task was to make predictions. None of it was done here. But the understanding og the relationship between the ED-demand and the temperature increased. It could be deduced that:
* Weekly cycles
* Yearly cycles
* EHF big unfluence 








