---
title: "Assignment2_submission"
author: "Gabor Wehrmuller - 219369109"
date: "07/09/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
library(mgcv)
library(rnoaa)
library(zoo)

```

# To do:

2.2 Check when the weather data is updated -
2.4 What exactly is asked - statistical methods? What is exactly used to estimate the parameters in GAMs 
3.4 report through coefficient estimates or plots?
3.6 but dont we specify before as well ordered weekdays?
tidy output with broom
3.6 check out what by does 

 
# Task 1 Source Weather Data

## 1. Which data source do you plan to use? Justify your decision.  

The original data stems from weather stations. They are usually run by the local government. However the collection and storing of the data can be done by different providers. I plan to use NOAA, the National Centers for Environmantal Information, an US-based data center. Also, I collect data from the Autralian Bureau of Meteorology. 

Additionaly, the data from last assignment, the ED-Demand stems from the Autralian government website. 

Generally government run databases are a good source, since they are free and reliable.  

## 2.

The relevant time period is not really defined. Therefore I assume the same time period as in Assignment 1.

```{r noaa_data}
# set key
options(noaakey = "aeiJGRXtNXdMuIkEVeqOvciRvsdBbIUg")

# get the id for perth
x <- ncdc_locs(locationcategoryid='CITY', sortfield='name', sortorder='desc',limit = 1000)$data
perth <- x[x$name=="Perth, AS",]
# id:	CITY:AS000008

# get the weatherstations and select the one with the highest coverage
stations <- ncdc_stations(datasetid='GHCND',
                          locationid='CITY:AS000008')$data
#x perth airport GHCND:ASN00009021

# check which datasets have perth data
ncdc_datasets(locationid = "CITY:AS000008")
# GHCND is here the only source

# the possible data are max, min, avg and precipitation
max_temp_daily <- ncdc(datasetid='GHCND',
                       stationid='GHCND:ASN00009021',
                       datatypeid='TMAX',
                       startdate = '2013-07-01',
                       enddate = '2014-06-30',
                       limit=500)$data
min_temp_daily <- ncdc(datasetid='GHCND',
                       stationid='GHCND:ASN00009021',
                       datatypeid='TMIN',
                       startdate = '2013-07-01',
                       enddate = '2014-06-30',
                       limit=500)$data
avg_temp_daily <- ncdc(datasetid='GHCND',
                       stationid='GHCND:ASN00009021',
                       datatypeid='TAVG', 
                       startdate = '2013-07-01',
                       enddate = '2014-06-30',
                       limit=500)$data
prec_daily <-ncdc(datasetid='GHCND',
                  stationid='GHCND:ASN00009021',
                  datatypeid='PRCP',
                  startdate = '2013-07-01',
                  enddate = '2014-06-30', 
                  limit=500)$data

# change date and select the important variables

change_noaadate <- function(df){
  require(tidyverse)
  df_2 <- df %>%
    dplyr::mutate(date = as.Date(substr(date,1,10)))%>%
    select(date,value)
  names(df_2) <- c("date",df$datatype[1])
  return(df_2)
}

max_temp_daily <- change_noaadate(max_temp_daily)
min_temp_daily <- change_noaadate(min_temp_daily)
avg_temp_daily <- change_noaadate(avg_temp_daily)
prec_daily <- change_noaadate(prec_daily)

dim(max_temp_daily)

```
## 3.


Each of the four (three for the daily temperatures, one for precipitaition) dataframes has 365 rows. I chose the time period to be the same as in the ed-demand data, which is 2013-07-01 to 2014-06-30. During the assignment this could also change, but for now this is what I'm working with.


 

# Task 2: Model planning

## 1.  

The model could play an important role for hospitals to control the ED-demand. Usually people have to wait for a long time till they get treatment at a hospital emergency departement. With an accurate prediction about the demand in hospitals, people could see in which hospital the demand is higher than in others and the patient could better chose where to go to minimise/optimise waiting time. 

Not only is it a problem, that patients are not well allocated, but also that often there are too at the ED, who are not seriously in danger. They could just make an appointment with a doctor next week. If people are more aware of the possible waiting times at ED, some of them might wait and make an normal appointment. 

Finally, this predictions can also be valuable for the hospitals personal planning. To better plan when it is suitable for nurses etc to take vacations/trainings and when there is high demand.

Overall this model can be used by the people for choosing the hospital with the least waiting time, or they see that theyjust wait for an appointment and finally also for personal planning at hospitals. 


## 2. 

As I have outlined in 1. There might be 2 applications. One, when the possible patient tries to decide to go to which hospital and another when the hospitals wants to make their planning more efficient. For the planning in a hospital, a prediction over a longer period of time is helpful. But for the possible patient, a daily prediction is what he wants. 

In this assignment, I want to focus only on the daily prediction for the patient. In this case the ED-Demand, the Attendance is the dependent variable. The predictors are the climate data from the day before. 

It seems that for a daily prediction, the NOAA database is not suitable

## 3.

It is known that climate data shows a reliable seasonality over the course over a year (the seasons). However, with increasing changes due to the climate change, this might not be true any longer. More and more extreme weather events happen. So there is no guarantee that in the future we will observe the same seasonalities/patterns. 

## 4. 

The dependent variable is numeric. Therefore, it will be a regression problem. By starting at a simple linear regression one can go on to more comoplex models if necessary. The methods used to estimate the parameters are in a linear regression the Ordinary Least Squares (OLS). When we would go on to the Generalized Addiditves Models, the method used for the parameter estimation would be the IRLS. 

# Task 3
## 1.

### ED-Data

```{r ed_data, message=FALSE, warning=FALSE}

# code from assignmnet 1
ed_link <- "http://bit.ly/2nkCUEh" #changed to link

top_row <- read_csv(ed_link, col_names = FALSE, n_max = 1)
second_row <- read_csv(ed_link, n_max = 1)

column_names <- second_row %>% 
  unlist(., use.names=FALSE) %>% 
  make.unique(., sep = "__") # double underscore

column_names[2:8] <- str_c(column_names[2:8], '0', sep='__')

daily_attendance <- 
  read_csv(ed_link, skip = 2, col_names = column_names,na = "N/A")


```


```{r tidy_ed}
# tidy the data
daily_attendance_tidy <-  daily_attendance %>%
  gather(-Date, key = key, value = value)%>% # gather all collumns except date
  tidyr::extract(key, c("category", "hospital_number"), "(\\w*)(\\d)", convert = TRUE)%>% # extract the hospital numbers
  mutate(category = str_sub(category,1,-3))%>%
       #  value = replace_na(value,0))%>%
  spread(category, value)%>%
  mutate(Hospital = case_when(hospital_number == 0 ~"Royal Perth Hospital",
                              hospital_number == 1 ~"Fremantle Hospital",
                              hospital_number == 2 ~"Princess Margaret Hospital For Children",
                              hospital_number == 3 ~"King Edward Memorial Hospital For Women",
                              hospital_number == 4 ~"Sir Charles Gairdner Hospital",
                              hospital_number == 5 ~"Armadale/Kelmscott District Memorial Hospital",
                              hospital_number == 6 ~"Swan District Hospital",
                              hospital_number == 7 ~"Rockingham General Hospital",
                              hospital_number == 8 ~"Joondalup Health Campus"))%>%
  dplyr::select(-hospital_number) # remove column
# replace all NAs in df with 0 - justification see Ass 1
daily_attendance_tidy[is.na(daily_attendance_tidy)] <- 0
```

## 1. 
```{r}
# to obtain the list of hospitals
set.seed(123)# to get the same results
hospitals <- top_row %>%
  select_if(~!is.na(.))

# print random Hospital
print(sample(as.vector(hospitals),size = 1))
```
Princess Margaret Hospital For Children is the selected hospital.
I assume that the next step is to filter the ED-date for this particlular hospital and continue with that data the assignment.

```{r}
Margaret_ed <- daily_attendance_tidy%>%
  filter(Hospital == "Princess Margaret Hospital For Children")%>%
  mutate(date = dmy(Date))

```





## 2. 
```{r}
fit_linear <- lm(Attendance~date, data = Margaret_ed) 
# linear regression
summary(fit_linear)

Margaret_ed%>%
  ggplot(aes(x = date,y = Attendance))+
  geom_point()+
  geom_smooth(method = "lm")

# 
# res <- stack(data.frame(Observed = Margaret_ed$Attendance , Predicted = fitted(fit_linear)))
# res <- cbind(res, x = rep(Margaret_ed$date, 2))
# 
# require("lattice")
# 
# xyplot(values ~ x, data = res, group = ind, auto.key = TRUE)
# 
plot(fit_linear$residuals)
# 

plot(fit_linear)
```
From the regression output, It can be seen that the linear fit is really bad. Only 0.3% of the variation can be explained. by "date". This means over the course over a year there is no linear trend 

The residualplots indicate that there is some seasonality that we are not capturing with this linear fit. So this linear regression is defenitely not good enough.

## 3. 
```{r}
fit_gam <- gam(Attendance~s(as.numeric(date), k = 50),
               data = Margaret_ed)
summary(fit_gam)
plot(fit_gam, residuals = TRUE)
gam.check(fit_gam)
```
The fit increased and much more can be explained. The residuals are nicely distributed around the estimate and the adjusted R-sqared is 0.412. But by looking at the residuals a bit closer, it seems that there is a repeated over-/underestimation. The distance between those point looks also the same. This could be an indication the there is a missed out (weekly) seasonality.


## 4.
 
```{r}
# extract the weekdays. It is not defined which type, therefore I 
# use the same (as.factor) as in the practical. The default is unordered
Margaret_ed_week <- Margaret_ed%>%
  mutate(weekd = as.factor(weekdays(date)),
         num_date  =as.numeric(date))

fit_gam_week <- gam(Attendance ~s(num_date, by  = weekd) +weekd,
                    data = Margaret_ed_week)
summary(fit_gam_week)

AIC(fit_gam,fit_gam_week)


plot(fit_gam_week, residuals = T)

```
The AIC reveals that the second model, fit_gam_week has a better fit than the first model. It incrorporates the weekly seasonalities. The coefficients show how different the coefficients are for the different days of the week. 

## 5. 


```{r}
plot(fit_gam_week, residuals = T)
gam.check(fit_gam_week)
```
The residual histogram looks acceptable and also the other plots do not give any signs that there would be another pattern in the resiudals. 

## 6. 
The variable is categorical. The default of as.factor is ordered = False. I assume that when we specify an ordered facor, then we loose some flexibility and thus have a worse fit. The numeric solution is also less flexible and should reseble the ordered one

```{r}
# add the ordinal  and numeric
Margaret_ed_week <- Margaret_ed%>%
   mutate(weekd = as.factor(weekdays(date)),
         weekd_or = factor(weekdays(date), order = T, levels = c("Monday","Tuesday","Wednesday","Thursday","Friday","Saturday","Sunday")),
         weekd_nu = lubridate::wday(date),
         num_date  =as.numeric(date))

# character
fit_gam_week <- gam(Attendance ~s(num_date, by = weekd) +weekd,
                    data = Margaret_ed_week)
summary(fit_gam_week)

# ordinal
fit_gam_week_o <- gam(Attendance ~s(num_date, by  = weekd_or) +weekd_or,
                    data = Margaret_ed_week)
summary(fit_gam_week_o)

# numeric
fit_gam_week_n <- gam(Attendance ~s(num_date, by  = weekd_nu) +weekd_nu,
                    data = Margaret_ed_week)
summary(fit_gam_week_n)



AIC(fit_gam_week,fit_gam_week_o,fit_gam_week_n)

```
There is a big difference in between the models. The presumably unordered model is the best, as outlined.

# Task 4 Heatwaves and ED demands 

## 1. The Definition of EHF
The EHF is calculated in multiple steps and tries to capture and indicate heatwaves.

First step is to calculate the excess heat indndices:

$EHI_{sig} = (T_i+T_{i+1}+T_{i+2})/3-T_{95}\;,where \;T\ is \;Daily\;Mean\;Temperature\; and \;T_{95} \;95 \; percentile$
and:
$EHI_{accl}=(T_i+T_{i+1}+T_{i+2})/3-(T_{i-1}+...+T_{i-30})/30 $

The EFH can be calculated as followed:

$EHF = EHI_{sign}*max(1,EHI_{accl})$
or
$EHF = max(0,EHI_{sign})*max(1,EHI_{accl})$


Knowing this, additional data is required:
* 30 additional days, so start is now 2013-06-01 and end 2014-07-02

* years 1971–200 to caclulate $T_{95}$


```{r merge_data}


# the possible data are max, min, avg and precipitation
# since it is not possible to request data longer than a year, this
# ugly concat is necessary for the 30 previous days. Im not doing this again only for the last two days of NAs
max_temp_daily_long <- data.frame(rbind(change_noaadate(ncdc(datasetid='GHCND',
                       stationid='GHCND:ASN00009021',
                       datatypeid='TMAX',
                       startdate = '2013-06-01',
                       enddate = '2013-06-30',
                       limit=500)$data),max_temp_daily))
min_temp_daily_long <- data.frame(rbind(change_noaadate(ncdc(datasetid='GHCND',
                       stationid='GHCND:ASN00009021',
                       datatypeid='TMIN',
                       startdate = '2013-06-01',
                       enddate = '2013-06-30',
                       limit=500)$data),min_temp_daily))
avg_temp_daily_long <- data.frame(rbind(change_noaadate(ncdc(datasetid='GHCND',
                       stationid='GHCND:ASN00009021',
                       datatypeid='TAVG', 
                       startdate = '2013-06-01',
                       enddate = '2013-06-30',
                       limit=500)$data),avg_temp_daily))
prec_daily_long <-data.frame(rbind(change_noaadate(ncdc(datasetid='GHCND',
                  stationid='GHCND:ASN00009021',
                  datatypeid='PRCP',
                  startdate = '2013-06-01',
                  enddate = '2013-06-30',
                  limit=500)$data),prec_daily))



#Not yet merging with margaret data - since then we would lose again the 30 days we added.
weather_long <- max_temp_daily_long%>%
  left_join(min_temp_daily_long,by = "date")%>%
  left_join(avg_temp_daily_long,by = "date")%>%
  left_join(prec_daily_long,by = "date")%>%
  arrange(date)
```

calculate EHF
```{r}
calc_t95 <- function(timesspan){
  require(rnoaa)
  avg_temp <-  c()
  for (i in timesspan){
    start <-  paste0(i,"-01-01")
    end <-  paste0(i,"-12-31")
    
    avg_temp_daily <- ncdc(datasetid='GHCND',
                         stationid='GHCND:ASN00009021',
                         datatypeid='TAVG', 
                         startdate = start,
                         enddate = end,
                         limit=500)$data
    if (length(avg_temp_daily)==0){
      print(paste("missed:",i))
      next
    }
    avg_temp <- append(avg_temp, avg_temp_daily$value)
  }
  return(quantile(avg_temp,.95))
  }


calc_ehf <- function(df,timespan,version = "positive"){
  require(zoo)
  df$EHF <- 1
  
  # EHI_sig
  t_95 <- calc_t95(timespan) # calculate 95 percentile
  # check to get the whole dataset of avg temperature to calculate it
  rolled_mean_f <- c(rollmean(df$TAVG,3),NA,NA) #= first try add NA at the end
  EHI_sig <- rolled_mean_f-t_95
  
  # EHI_accl
  rolled_mean_b <- c(rep(NA,30),
                     rollmeanr(lag(df$TAVG),30))
 EHI_accl <- rolled_mean_f - rolled_mean_b
 if (version == "positive"){
  for (i in 1:nrow(df)){
    df$EHF[i] = max(0,EHI_sig[i])*max(1,EHI_accl[i])
    }
 }
 if (version == "negative"){
  for (i in 1:nrow(df)){
    df$EHF[i] = EHI_sig[i]*max(1,EHI_accl[i])
    }
 }
 
 return(df) 
}
```


```{r}

timespan <- 1980:2010

# weather_EHF_positive <- calc_ehf(weather_long, timespan, version = "positive")
weather_EHF_negative <- calc_ehf(weather_long, timespan, version = "negative")


# ggplot(weather_EHF, aes(x = date, y = EHF))+geom_line()

ggplot() + 
geom_line(data=weather_EHF_positive, aes(x = date, y = EHF), color='green') + 
geom_line(data=weather_EHF_negative, aes(x = date, y = EHF), color='red')



```
The "negative" EHF has more variation - therefore I will include this one in the model


## Task 4.2 

```{r}
margaret_weather_EHF <-  Margaret_ed_week%>%
  left_join(weather_EHF_negative, by = "date")

fit_gam_ehf <- gam(Attendance ~s(num_date, by = weekd) +weekd+s(EHF),
                    data = margaret_weather_EHF)
summary(fit_gam_ehf)

AIC(fit_gam_week,fit_gam_ehf)

plot(fit_gam_ehf)

```


The EHF increases the fit again. The model performs better and can better explain the variation, as the AIC and the adj R-square confirm. 
The coefficient of EHF increasis with increasing EHF.

The interpretation is that in heatwaves, we have more demand for ED. This covers with the expectations, which are that during heatwaves especially elderly people don't drink enough or have otherwise problems with the heat and therefore there is a higher demand for ED. 



## Task 4.3

An extra variable would be the solar data. When the sun is directly shining on people, it can be that more people get sunburnt or have a sunstroke and need to get to the ED.

```{r sun_data}
sun_daily <- read_csv("C:/Users/gwehrm/Documents/Repos/Deakin/Statistical/Data/IDCJAC0016_009021_1800_Data.csv")

sun_daily <- sun_daily%>%
  unite(date ,c(Year,Month,Day), sep = "-",remove = T)%>%
  mutate(date = as.Date(date))%>%
  filter(between(date, as.Date("2013-07-01"),as.Date("2014-06-30")))%>%
  select(date, `Daily global solar exposure (MJ/m*m)`)%>%
  rename(solar = `Daily global solar exposure (MJ/m*m)`)

margaret_weather_s <- margaret_weather_EHF%>%
  left_join(sun_daily, by = "date")%>%
  arrange(date)


fit_gam_s <- gam(Attendance ~s(num_date, by = weekd) +weekd+s(EHF)+s(solar)+s(PRCP),
                    data = margaret_weather_s)
summary(fit_gam_s)

AIC(fit_gam_ehf,fit_gam_s)

plot(fit_gam_s)


```
It barely improves the fit of the model. this might be due to the fact, that EHF covers this already.


# Task 5
## 1 limitations of historical data

The Example of the EHF can show some limitations. With increasing temperature more of the data will be labelled as heatwave, since as comparison historical data is used. So, at least for a while, the years will have maybe an overrepresentation of EHF. Since meanwhile people could adapt themselves to the increased heat and be therefore be less sensitive to the EHF callculated with historical data.

## 2. 
Well originally the task was to predict the ED-demand. But given multiple factors, the understanding of the process should be more in the focus. One reason is that only data for a year is available. Also, the data is from 2014. As said in 1., a limitation of historical data is that the behaviour and the relation can change. The data is nearly six years old and therefore not suitable to use for predictions in 2019/2020.

But insights forom this models can be used to the incorporate features in the prediction model.

## 3.
No not overall. The original task was to make predictions. None of it was done here. But the understanding og the relationship between the ED-demand and the temperature increased. It could be deduced that:
* Weekly cycles
* Yearly cycles
* EHF big influence 