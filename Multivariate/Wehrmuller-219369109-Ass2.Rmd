---
title: "Wehrmuller-219369109-Ass2"
author: "Gabor Wehrmuller"
date: "22/09/2019"
output: pdf_document
bibliography: library.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(igraph)
library(ggm)

```

# Q1)

## 1.1

$P(J,W,S,A,P,V,D,U,M) = P(J)P(A)P(W|J)P(P|W)P(U|P,D)P(M|U)P(S|J)P(V|S,A)P(D|V)$

## 1.2

For each node: $(n_v-1)\prod \limits_{u}^{}n_u$

$P(J)=3$  

$P(A)=3$  

$P(W|J)=1*4=4$  

$P(P|W)=1*2=2$  

$P(U|P,D)=2*8=16$  

$P(M|U)=3*3=9$  

$P(V|S,A)=1*16=16$  

$P(D|V)=3*2=6$  


Total: 59

## 1.3

Without the assumption:

$(2^3-1)(3^1-1)(4^5-1)= 14322$

Thanks to the assumptions about the independence among the variables, the numbers of necessary parameters can drastically be reduced. This makes inference less complex and more intuitive. 

## 1.4
### 1.4 a)

$W \bot | \emptyset$  

possible paths:  

$\{W,J,S,V\},\{W,P,U,D,V\}$  

The first path is unblocked, the second is by U. Therefore they are not d-separated and W is not marginally independent of V. The statement is **false**. 

### 1.4 b)

$A \bot M | \{D,W\}$  

possible paths:  

$\{A,V,S,J,W,P,U,M\}, \{A,V,D,U,M\}$  

The first path is blocked, since W is observed and blockes the path. The second path is blocked, since D is observed and blockes the path. The statements is **true**. 


### 1.4 c)

$\{A,W \} \bot D|V$  

possible paths:   

$A:\{ A,V,D\},\{A,V,S,J,W,P,U,D \} \\ W: \{W,P,U,D \},\{W,J,S,V,D\}$  

The first path for A is blockked, since V is observed and blockes the path. The second path for A is blocked by U, since there the paths are head-to-head. The head-to-head at V is unblocked, since V is observed.  


The first path for W is blocked by U since there the paths are head-to-head. The second path is blocked, since V is observed and changes the path from unblocked to blocked. 
  
The statement is **true**.


## 1.5 

```{r}

dag <- DAG(W~J, P~W, U~P+D,M~U, S~J, V~S+A, D~V)

drawGraph(dag, adjust = F)

# a)
dSep(dag,first = "W", second = "V", cond = c())

# b)
dSep(dag,first = "A", second = "M", cond = c("D","W"))

# c)
dSep(dag,first = c("A","W"), second = "D", cond = c("V"))

```

\newpage
## 1.6

$P(M|S=3000-6000,V=Yes, P=Peakhour,D = <1)\\Order: J,W,A,U\\ P(M|S=3000-6000,V=Yes, P=Peakhour,D = <1) = \frac{P(M, S=3000-6000,V=Yes, P=Peakhour,D = <1)}{P(S=3000-6000,V=Yes, P=Peakhour,D = <1)}\\P(M,S,V,P,D= \sum_{J,W,A,U}P(J)P(A)P(W|J)P(P|W)P(U|P,D)P(M|U)P(S|J)P(V|S,A)P(D|V)\\ =\sum_{J,W,A,U}f_0(A)f_1(J)f_2(W,J)f_3(P,W)f_4(V,P,D)f_5(M,U)f_6(S,J)f_7(V,S,A)f_8(D,V)\\Observe\; S=3000-6000,V=Yes, P=Peakhour,D = <1\\=\sum_{J,W,A,U}f_0(A)f_1(J)f_2(W,J)f_9(W)f_5(M,U)f_{10}(J)f_{11}(A)\\Eliminate \;J\\=\sum_{W,A,U}f_0(A)f_9(W)f_5(M,U)f_{11}(A)\sum_Jf_1(J)f_2(W,J)f_{10}(J)\\ = \sum_{W,A,U}f_0(A)f_9(W)f_5(M,U)f_{11}(A)f_2(W)\\Eliminate\;W\\ =\sum_{A,U}f_0(A)f_5(M,U)f_{11}(A)\sum_Wf_9(W)f_2(W)\\=\sum_{A,U}f_0(A)f_5(M,U)f_{11}(A)\\Eliminate \;A\\= \sum_Uf_5(M,U)\sum_Af_9(A)f_2(A)\\=\sum_Uf_5(M,U)\\Eliminate\;U\\=f_5(M)\\The \;posterior \;distribution:\\P(M|S=3000-6000,V=Yes, P=Peakhour,D = <1)=\frac{f_5(M)}{\sum_Mf_5(M)}$


# 2. 
## 2.1 a)
```{r 2.1, message=FALSE, warning=FALSE}
# biocLite is not available for r 3.6 - therefore usage of BiocManager to install the packeages
library(RBGL)
library(gRbase)
library(gRain)
library(Rgraphviz)

lh <- c("low","high")
lmmh <- c("low","lower middle","upper middle","high")

eh <- cptable(~eh, values=c(3, 7), levels=lh) 

oil.eh <- cptable(~oil|eh, values = c(3,7,6,4),levels = lh)

bp.oil <- cptable(~bp|oil, values = c(20,45,30,5, 20,50,20,10),levels = lmmh)

inf.oil.eh <- cptable(~inf|oil:eh, values = c(1,9,  6,4,  5,5,  4,6),levels = lh)

rt.inf.eh <- cptable(~rt|inf:eh, values = c(2,8,  6,4,  7,3,   2,8), levels = lh)

plist <- compileCPT(list(eh, oil.eh,bp.oil,inf.oil.eh,rt.inf.eh))
net <- grain(plist)
plot(net)


```

## 2.1 b)
```{r}
for (i in plist){
  print("--------------------")
  print(i)
}
```
They are the same as in the table. 


## 2.2 a)
```{r}
net_2 <- setEvidence(net,nodes = c("oil","ret"),states = c("low","low"))
querygrain(net_2,nodes = "bp")
```
The most likely state for the British petroleum stock price is "middle" with a probability of 45%. 

## 2.2 b)
```{r}
net_3 <- setEvidence(net,nodes = "inf",states = "low")
querygrain(net_3,nodes = "rt")
```
the probability that the retailer stock pris is high is 44.77%

## 2.2 c)
```{r}
querygrain(net, nodes = "oil", type = "marginal")
```

## 2.2 d)
```{r}
querygrain(net, nodes = c("inf", "bp"), type = "joint")
```

# Q3

## 3.1)
$P(A,B)=\sum_{C,E,D}P(A,B,C,D,E)\\=\sum_{C,E,D}P(A)P(B)P(C|A,B)P(E|C)P(D|C)\\=\sum_{E,D}P(A)P(B)\sum_{C}P(C|A,B)P(E|C)P(D|C)\\=\sum_{E,D}P(A)P(B)P(E)P(D)\\=P(A)P(B)\sum_EP(E)\sum_DP(D)\\=P(A)P(B)$  

A and B are d-separated by C, since the path is blocked by head-to-head edges. 

## 3.2)

![](20190925_135804.jpg)

The result of this page (Numerator) is $0.96-0.4\beta$

![](20190925_135809.jpg)

The result of the Denominator is 2.

Resuts in:  

$\frac{0.96-0.4\beta}{2}=\underline{0.48-0.2\beta}$


## 3.3)

$\alpha= P(A=0)=\frac{\#(A=0)}{N}=\frac{3}{20}=0.15$  

$\beta= P(E=0|C=0)\frac{\#(C=0,E=0)}{\#(C=0)}=\frac{5}{12}=0.416$  

$\gamma = P(D=0|C=0=\frac{\#(D=0,C=0)}{\#(C=0)}=\frac{2}{12}=0.16$  

$\theta = P(B=0) = \frac{\#(B=0)}{N}=\frac{4}{20}=0.2$  


## 3.4)

$P(E=1|A=1,B=0)=0.48-0.2\beta\\=0.48-0.2*416=0.396$  


# Q4

```{r}
ChildData <- read.csv(file="CHILD10k.csv", header=TRUE, sep=",")

library(bnlearn)
#create and plot the network structure.
modelstring = paste0("[BirthAsphyxia][Disease|BirthAsphyxia][LVH|Disease][DuctFlow|Disease]",
"[CardiacMixing|Disease][LungParench|Disease][LungFlow|Disease][Sick|Disease]",
"[HypDistrib|DuctFlow:CardiacMixing][HypoxiaInO2|CardiacMixing:LungParench]",
"[CO2|LungParench][ChestXray|LungParench:LungFlow][Grunting|LungParench:Sick]",
"[LVHreport|LVH][Age|Disease:Sick][LowerBodyO2|HypDistrib:HypoxiaInO2]",
"[RUQO2|HypoxiaInO2][CO2Report|CO2][XrayReport|ChestXray][GruntingReport|Grunting]")
dag = model2network(modelstring)
par(mfrow = c(1,1))
#source("https://bioconductor.org/biocLite.R")
#biocLite("Rgraphviz")
graphviz.plot(dag)
```

## 4.1)

```{r}
for (i in c(100,500,1000,5000)){
  bnet_bic = hc(ChildData[1:i,], score = "bic") 
  bnet_bde = hc(ChildData[1:i,], score = "bde")
  score_bic = bnlearn::score(bnet_bic, ChildData[1:i,], type = "bic")
  score_bde = bnlearn::score(bnet_bde, ChildData[1:i,], type = "bde")
  
  graphviz.plot(bnet_bic, main = paste("\nBayesian Network (score = BIC) with the first", i, "Observations"), sub = paste("BIC-Score:",score_bic))
  graphviz.plot(bnet_bde, main = paste("\nBayesian Network (score = BDE) with the first", i, "Observations"), sub = paste("BDE-Score:",score_bde))

}



```

## 4.2)

The BIC score increases linearily with sample size. With more samples, the incentive to fit the exact structure of the data increases. This means that with increasing data size, the structure gets more complex, which lowers the score again. This can be observed as well in the plots.

The BDE scores increases as well with sample size. With low sample size, the score tends to favor simpler structures. As more data can be used, more complex structures are also taken into cionisderation. This can be seen that again with more data, the structure gets more complex. 

### 4.3 a)
```{r}
bnet.bic.full <- hc(ChildData, score = "bic")
bnet.bde.full <- hc(ChildData, score = "bde")

score.bic.full = bnlearn::score(bnet.bic.full, ChildData, type = "bic")
score.bde.full = bnlearn::score(bnet.bde.full, ChildData, type = "bde")
  
graphviz.plot(bnet.bic.full, main = paste("\nBayesian Network (score = BIC)"), sub = paste("BIC-Score:",score.bic.full))
graphviz.plot(bnet.bde.full, main = paste("\nBayesian Network (score = BDE)"), sub = paste("BDE-Score:",score.bde.full))



```

### 4.3 b)
```{r}

par(mfrow = c(1,2))
for (i in c(100,500,1000,5000)){
  assign(paste0("bnet.bic",i), hc(ChildData[1:i,], score = "bic")) 
  assign(paste0("bnet.bde",i),hc(ChildData[1:i,], score = "bde"))
  }

  # compare(bnet.bic.full,bnet.bic)
graphviz.compare(bnet.bic.full,bnet.bic100,bnet.bic500,bnet.bic1000,bnet.bic5000,main = c("Full (BIC)", "100", "500","1000","5000"))
  
  
  # compare(bnet.bde.full,bnet.bde)
graphviz.compare(bnet.bde.full,bnet.bde100,bnet.bde500,bnet.bde1000,bnet.bde5000,main = c("Full (BNE)", "100", "500","1000","5000"))
  
```

Incentive is in both cases to draw less complex structures with limited date. Hence, as observed, with increasing number of datapoints the differences between the full and the sub-models decrease. 



### 4.3 c) 

```{r}
fitted.param = bn.fit(bnet.bic.full, ChildData)
fitted.param
```

### 4.3 d)

```{r}
cpquery(fitted.param, event = (Disease =="Lung"), evidence = ((CO2=="High") & (LungParench =="Abnormal")))

```
The probability is 0.17



# Q5)

A Bayesian Network is used by the US Military for Compat Equipment Diagnostics. The military, or the be more precise the man in the field face the problem that at certain times they need to act quick and need to fully rely on their equipment. So, in case a vital element, like a truck, needs maintenance, this needs to be done as quick as possible. An expereinced technician, that can easily repair this might not arrive within days. So, a causal Bayesian Network (DAG) that holds expert knowledge was developed. It helps and gives guidance for the soldiers to repair the equipment. This method was chosen, since it allows to easily illustrate complex relationships and visualises the causal path structures easily understandable @Aebischer2017. 


In the second application, the bayes theorem is used to evaluate the performance of depression tests. It is described that the usage of frequentists tests underestimates the possibility of misclassifications based on cutoffs. By using the Bayes theorem they evaluate the tests newly and get the results that besides one test, none of them reaches a statisfactory level of diagnostic accuracy. The best test uses a newer procedure to select the best items. This paper encourages to use this method in the future to select the items and have a higher diagnostic accuracy @Tommasi2018.  

# References
